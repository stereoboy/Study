{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* Stanford NLP Lecture\n",
    "  * http://cs224d.stanford.edu/syllabus.html\n",
    "  \n",
    "# Data\n",
    "\n",
    "* babi data for\n",
    "  * http://www.thespermwhale.com/jaseweston/\n",
    "    * http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Networks Facebook AI\n",
    "## Memory Networks (2014)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-To-End Memory Networks\n",
    "* Source Code\n",
    "  * https://github.com/facebook/MemNN\n",
    "* Differential Version Of Memory Networks\n",
    "* Two grand challenges in artifical intelligence research\n",
    "  * Multiple computational steps in the service of answering a question or completing a task\n",
    "  * Long term dependencies in sequential data\n",
    "* Because the function from input to output is smooth, we can easily compute gradients and back-propagate through it.\n",
    "<img  src=\"C2Q7W6NKPYUTPK448GGVG6E7W1VXQ408.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTM DeepMind\n",
    "## Neural Turing Machine (2014)\n",
    "\n",
    "* Human Cognition VS Computing\n",
    "  * Rule-based manipulation VS Simple Program\n",
    "  * Short-term storage of information VS Program arguments  \n",
    "  * -> Working Memormy VS \"NTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"LKTTEJA7F9N1D58DRU1G1Q1AF3X0JUTB.png\"/>\n",
    "<img  src=\"YPIE6LGFCS12CPEWOSRG7IB2GXC0C831.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* !!! bit #7, bit #8 in input are delimiter bits\n",
    "  * On bit #7 means \"input start\"\n",
    "  * On bit #8 means \"input end and start result\" such as <go> word in seq2seq\n",
    "  * insert zero bits in inputs while outputs are generated\n",
    "<img  src=\"B61HT3HBUA6KGAIV6NGR3HLLVRVAOAGL.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING NEURAL TURING MACHINES\n",
    "* source code \n",
    "  * https://github.com/ilyasu123/rlntm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid computing using a neural network with dynamic external memory (DNC) (2016)\n",
    "* Unofficial Source code\n",
    "  * https://github.com/Mostafa-Samir/DNC-tensorflow\n",
    "* NTM vs DNC\n",
    "  * Same at target level\n",
    "  * Implementation of addressing method + introduction of memory allocation method\n",
    "  * DNC is better for accuracy (comparison in bAbI task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\n",
    "\n",
    "* SAM (Sparse Access Model)\n",
    "  * Upgrade NTM using ANN for large size external moemory \n",
    "    * \"Scaling\" = Large size\n",
    "    * Approximate Nearest Neighbor (ANN) $\\mathcal{O}(\\log N)$ instead of linear search $\\mathcal{O}(N)$\n",
    "    * K-Nearest Neighbor(KNN)\n",
    "      * K sparse number\n",
    "  * c.f. Sparse Differentiable Neural Computer (SDNC) is upgrade version of DNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"ENHTL692NDDOTXHP6K151W8513729XEC.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointer Networks (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Me Anything: Dynamic Memory Networks for Natural Language Processing (DMN, Dynamic Memory Networks) (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Hierarchical Memory Networks\" (ICLR 2017)\n",
    "Reduce memory seen by softmax by expressing memory hierarchically\n",
    "\n",
    "## \"Dynamic NTM with Continuous and Discrete Addressing Schemes\" ICLR 2017\n",
    "Based on REINFORCE hard attention and softmax\n",
    "Soft attention is used in combination\n",
    "I do not really understand\n",
    "\n",
    "## \"Lie Access Neural Turing Machine\" ICLR 2017\n",
    "Addressing using Lee group\n",
    "I can move the head naturally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
