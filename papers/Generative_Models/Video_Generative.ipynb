{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Image Translation with Conditional Adversarial Networks\n",
    "* BAIR, UC Berkeley\n",
    "### Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video-to-Video Synthesis\n",
    "* Nvidia, MIT, 2018\n",
    "\n",
    "### Background\n",
    "* Image-to-Image Translation\n",
    "  * Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality.\n",
    "\n",
    "### Main Strategy\n",
    "* with a spatio-temporal adversarial objective\n",
    "  * Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coher- ent video results on a diverse set of input formats including segmentation masks, sketches, and poses.\n",
    "* future video prediction\n",
    "  * Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
