{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "* Tools\n",
    " * Ch13. Linear Factor Models\n",
    " * Ch14. Autoencoders\n",
    "* Overviews(?)\n",
    " * Ch15. Representation Learning\n",
    "* Specific Issues\n",
    " * Ch16. Structured Probabilistic Models for Deep Learning\n",
    " * Ch17. Monte Carlo Methods\n",
    " * Ch18. Confronting the Partition Function\n",
    " * Ch19. Approximation Inference\n",
    " * Ch20. Deep Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Introduction\n",
    " * Definition of Representation\n",
    "* Greedy Layer-Wise Unsupervised Pretraining\n",
    " * When and Why Does Unsupervised Pretraining Work?\n",
    "* Transfer Learning and Domain Adaptation\n",
    " * Use shared representation\n",
    "* Semi-Supervised Disentangling of Casual Factors\n",
    " * Use information from unsupervised tasks to perform supervised task\n",
    "* Distributed Representation\n",
    "* Exponential Gains from Depth\n",
    " * Deep representation\n",
    "* Providing Clues to Discover Underlying Causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Representation\n",
    "* Arabic numeral representation VS Roman numeral representation\n",
    " * 210 / 6  VS  CCX / VI\n",
    "* Better representation in Machine Learning\n",
    " * Good one makes a subsequent task easier\n",
    "* Almost learning algorithms learns \"Representations\" in the Deep Architecture\n",
    " * Supervised/Unsupervised Learning learns \"implicitly\" as side effects\n",
    " * Some algorithms designed explicitly for Representation Learning\n",
    "   * e.g. Distribution Learing (Density Estimation)\n",
    "* Tradeoff Issue\n",
    " * Preserving much information VS Nice properties (e.g. Independence)\n",
    "\n",
    "##  Use Unlabeled Data for a good representation\n",
    "* Unsupervised Learning\n",
    "* Semi-suprevised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.1 Greedy Layer-Wise Unsupervised Pretraining\n",
    "\n",
    "참고 이미지: https://wikidocs.net/images/page/3413/glw.png (출처: https://wikidocs.net/3413)\n",
    "\n",
    "* Greedy Layer-Wise?\n",
    " * optimizes each layer at a time rather than jointly optimizing all pices\n",
    " \n",
    "* Use single-layer representation learning algorithm\n",
    " * RBM, single-layer autoencoder, sparse coding model (Ch13/14)\n",
    " * Take the output of the previous layer\n",
    " * Produce a new simpler representation\n",
    "\n",
    "<img src='Algorithm_15_1.png' width=800>\n",
    "\n",
    "* Good Initialization for a joint learning procedure over all the layers of a deep neural net for supervised task\n",
    "* Used to successfully train \"even\" fully connected architectures\n",
    "\n",
    "\n",
    "* Fine tuning after pretraining\n",
    " * Optimizes all layers together\n",
    " * Can be done in the pretraining phase (pretraining & fine-tuning simultaneously)\n",
    "* Can be viewed as a regularizer in supervised learning task\n",
    "* Overall training scheme is nearly the same\n",
    " * learning algorithms, model types can differ\n",
    "\n",
    "\n",
    "* Initialization for unsupervised learning algorithms for...\n",
    " * Deep autoencoders\n",
    " * Probailistic models with many layers of latent variables\n",
    " * Deep Generative Models (Ch20)\n",
    "   * Deep belief networks\n",
    "   * Deep Boltzmann machines\n",
    "   \n",
    "## 15.1.1 When and Why Does Unsupervised Pretraining Work?\n",
    "\n",
    "* History\n",
    " * Substantial improvements in test error for \"Classification Tasks\"\n",
    "   * Revival of deep neural networks (2006, Hinton)\n",
    " * Harmful on many other tasks\n",
    " * Ma,J. (2015, Deep neural nets as a method for quantitative sturucture) found...\n",
    "   * Significantly helpful for many tasks\n",
    "   * Slightly harmful on average\n",
    " * So we should know \"When and Why pretraining works\" for a particular task\n",
    "\n",
    "\n",
    "* 2 Intuitions\n",
    "  * Act as regularizer\n",
    "    * e.g. Optimize only higher layers(classifier) freezing lower layers (feature extractor) \n",
    "    * Prevent overfitting\n",
    "    * Improve test set error\n",
    "    * Speed up optimization\n",
    "  * Some features that are useful for the unsupervised task may also be useful for the supervised learning task\n",
    "    * After extracting wheels, we can classify cars and motorcycles by counting wheels\n",
    "\n",
    "* Expected Values\n",
    " * More effective when the initial input is poor\n",
    "   * dimension reduction + manifold learning (Ch14)\n",
    "   * e.g. good similarity metrics between two words for word embeddings\n",
    " * User unlabeled data when labeled data is very small (Semi-supervised learning)\n",
    " * Regularization for complicated functions\n",
    " \n",
    " \n",
    "* Why it works\n",
    " * reduce the viraince of the estimation process\n",
    "   * Figure 15.1 explanation\n",
    "     * Input-output projection for visualization \n",
    "     * variaous starting points (initialization)\n",
    "     * blue -> red: time line, from origin to outside\n",
    "     * points based on pretraing move to small region\n",
    "\n",
    "<img src='Figure_15_1.png' width=800>\n",
    "\n",
    "* Comparison to other ways\n",
    " * Two \"separate\" phases\n",
    "   * Increase hyperparameters => time consuming\n",
    " * => one phase pretraining\n",
    "   * Unsupervised learning and supervised learning simultaneously\n",
    "   * Attach unsupervised learning term to objective function\n",
    "* Two phase VS one phase\n",
    " * many hyperparams vs single hyperparam\n",
    " * several trial-error iteration vs one-shot\n",
    " * no way to control regularization term vs control it by the coefficient of unsupervised cost term \n",
    "\n",
    "* The popularity of unsupervised pretraining has declined\n",
    " * Still popular in NLP(natural language processing)\n",
    " * Regualized with dropout or Batch normalization for classification\n",
    "   * outperform pretraing versions on even medium-size datasets\n",
    " * Bayesian methods outperform on small datasets\n",
    " \n",
    " \n",
    " \n",
    "* Nevertheless unsupervised pretraining...\n",
    " * an important milestone in the history of deep learing research\n",
    " * continues to influence contemporary approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.2 Transfer Learning and Domain Adaptation\n",
    "\n",
    "* One example problelm of Transfer learning\n",
    " * How to use feature extractor from Zebra vs Horse for classification of Dalmatian vs Dog\n",
    " \n",
    "* In transfer learning, the learner must perform two or more different tasks\n",
    " * e.g. Learn on significantly more data (P1), apply the learned transformation on P2(Small data)\n",
    " \n",
    "\n",
    "* Sharing layers\n",
    " * Share lower layers (Underlying factor in low level feature) => Multi-task learning\n",
    "   * e.g. Visual categorizing\n",
    "     * low-level notions of \"Edges\" and \"Visual shapes\" (corner? circle?)\n",
    " <img src='Figure_7_2.png' width=800>\n",
    " * Share higher layers (Speech recognitoin) => Domain Adaptation\n",
    " <img src='Figure_15_2.png' width=800>\n",
    " \n",
    "* Domain Adaptation (Sharing Higher Layer)\n",
    " * Same task -> Different distrtibution P\n",
    "  * e.g. Learning positive/Negative sentiment\n",
    "    * Task1: about Music, Task2: about Movies\n",
    "    * Why?: vocabulary and style vary from one domain to another\n",
    "\n",
    "\n",
    "* Concept Drift\n",
    " * Gradual changes in the data distribution over time\n",
    " \n",
    "---\n",
    "```\n",
    "While the phrase \"multi-task learning\" typically refers to supervised learning tasks, the more general notion of transfer learning is applicable to unsupervised learning and reinforcement learning as well.\n",
    "```\n",
    "----\n",
    "\n",
    "* Same representation may be useful in both settings\n",
    " * e.g. Transfer learning competition\n",
    "   * Mesnil, G. 2011, Unsupervised and tranfer learning challenge: a deep learing approach\n",
    "   * 1st: Learn on $P_1$\n",
    "   * 2st: Apply the learned transformation to $P_2$\n",
    "   * Result\n",
    "     * deeper representations => faster learning $P_2$\n",
    "\n",
    "* Two examples: One-shot learning and zero-shot(zero-data) learning\n",
    " * Extreme forms of transfer learning\n",
    " * One-shot: One example in the 2nd stage\n",
    "   * e.g. \n",
    "     * learn \"wheels\" from images of bikes n cars\n",
    "     * learn the one image of a 3-wheel bike\n",
    "     * test on images of 3-wheel bikes\n",
    " * Zero-shot\n",
    "   * Testing without data in the 2nd stage???\n",
    "   * Learn 2 representations and their relation\n",
    "   * e.g. Text-Image learning\n",
    "     * Link text space(\"4 Legs\") - Image space(visual shape of legs and their count)\n",
    "     * Learn Birds(\"2 Legs\", \"No Ear\"), Dogs(\"2 Legs\", \"Round Ears\")\n",
    "     * Input: Text about Cats (4 Legs, Pointy ears)\n",
    "     * Apply to the images of Cats\n",
    "   * e.g. Machine translation\n",
    "     * We can translate sentences even though some word has no label\n",
    "     * X in language A - Y in language B have similar behavior => Same meaning    \n",
    " \n",
    " \n",
    " <img src='Figure_15_3.png' width=600>\n",
    "\n",
    "\n",
    "* Zero-shot Model\n",
    " * $P(y| x, T)$\n",
    "   * Traditional input $x$\n",
    "   * Traditional output $y$\n",
    "   * Additional random variables, Task $T$\n",
    "   * e.g. $x$ is descriptions about cats, $y$ is \"yes\" or \"no\", $T$ is \"Is there a cat in this image?\"\n",
    " \n",
    "    ---\n",
    "    ```\n",
    "If we have a training set containing unsupervised examples of objects that live in the same space as T , we may be able to infer the meaning of unseen instances of T.\n",
    "    ```\n",
    "    ---\n",
    "\n",
    "    * $T$ should be represented in a way that allows some of generalization.\n",
    "      * \"Is there a sort of \"animals\" in this image?\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.3 Semi-Supervised Disentangling of Causal Factors\n",
    "* Large amount of unlabeled data and relatively little labeled data\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png width=300>\n",
    "\n",
    "* $P(x)$ is helpful for $P(y|x)$\n",
    "* Causal Factor -(Representation)-> Feature\n",
    "\n",
    "* Better Representations?\n",
    "  1. Representation disentangles the causes from one another\n",
    "  2. Easy to model\n",
    "    * e.g. Simple model: sparsity, independence\n",
    "  \n",
    "* Hypothesis motivation of Semi-supervised learning\n",
    "  * If (1), (2) conside =>\n",
    "  * If a representation $h$ represents many of the underlying causes of the observed $x$\n",
    "    * the outputs $y$ are among the most \"salient\" causes, then it is easy to predict $y$ from $h$.\n",
    "    * $P(y|x)$, $P(x|h)$, $P(h)$\n",
    "  * c.f. If $P(x)$ is uniformly distributed => Semi-supervised learning fails\n",
    "  * Simple example\n",
    "\n",
    "<img src=\"Figure_15_4.png\" width=600>\n",
    "  \n",
    "\n",
    "* Issus: Hard to capture salient factors\n",
    "  * Two Strategy\n",
    "    1. Use a supervised learning signal (labeld data)\n",
    "    2. Use much larger representation\n",
    "    \n",
    "    \n",
    "* Adversarial Framework (CH 20)\n",
    "  * Modify the definition of which underlying causes are most salient.\n",
    "  \n",
    "  \n",
    "<img src=\"Figure_15_6.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 15.4 Distributed Representation\n",
    "\n",
    "  <img src=\"Figure_15_8.png\" width=400>\n",
    "  \n",
    "* Symbolic Representation\n",
    "  * N features -> N Symbol or categories \n",
    "  * e.g.\n",
    "    * Red Car, Green Car, Blue Car, Red Truck, Green Truck, Blue Truck, Red Bird, Green Bird, Blue Bird\n",
    "  * N \"Binary\" features => One hot representation\n",
    "    * e.g.\n",
    "       * Red Car = [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "       * Blue Bird = [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    * Still \"Sparse\" Representation (CH 1)\n",
    "* Distributed Representation?\n",
    "  * e.g.\n",
    "    * Red Car = [[1, 0, 0], [1, 0, 0]] ([[Red Bit, Green Bit, Blue Bit],[Car Bit, Truck Bit, Bird Bit]])\n",
    "    * Blue Bird = [[0, 0, 1], [0, 0, 1]]\n",
    "  * Not all values are feasible\n",
    "    * e.g. [1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0] are not feasible\n",
    "  <img src=\"Figure_15_7.png\" width=600>\n",
    "  \n",
    "* One-hot representation VS Distribution Representation\n",
    "  * Only One entry can be active VS Multiple entry can be active\n",
    "  * Representation Dimension\n",
    "    * $n^d$ VS $n\\times d$\n",
    "      * $d:= \\text{Input Dim or Feature Dim}$, $n:= \\text{Feature Value Dim}$\n",
    "      * e.g. $3^2$ VS $2\\times3$\n",
    "    * Not Powerful VS Powerful\n",
    "* The combination of a Powerfule Representation Layer and a Week Classifier Layer can be a strong regularizer\n",
    "  * A classifier trying to learn the concept of \"person\" vs \"not a person\" does not need to assign a different class to an input represented as \"woman with glasses\".\n",
    "    * (person vs not a person), (man vs woman), (with glasses vs without glasses)\n",
    "  * This capacity constraint encourages each classifier to focus on few $h_i$ and encourages $\\mathbf{h}$ to learn to represent the classes in a linearly separable way\n",
    "    * some classifier focuses (man vs woman), another one focuses (with glasses vs without glasses)\n",
    "* Non-distributed Representation\n",
    "  * Example Type 1: Input point is assigned to exactly one cluster.\n",
    "    * Clustering methods: K-means algorithm\n",
    "    * Decision Trees\n",
    "  * Example Type 2: Entries cannot be controlled separately from each other.\n",
    "    * K-nearest neighbors algorithms\n",
    "    * Gaussian Mixtures and Mixtures of Experts\n",
    "    * Kernel machines with a Gaussian kernel\n",
    "  * ??? \n",
    "    * N-grams + Tree of suffixes (CH 12)\n",
    "    <img src=http://recognize-speech.com/images/Antonio/Unigram.png width=400>\n",
    "* Distibuted Represetation\n",
    "  * Generalization arises due to \"shared attributes\"\n",
    "    * \"cat\" and \"dog\"\n",
    "      * \"has_fur\" or \"number_of_legs\" have same value for the embedding of both.\n",
    "  * Induce a rich \"Similarity Spacee\"\n",
    "    * Semantically close concepts are close in \"Distance\"\n",
    "      * \"cat\" is closer to \"dog\" than \"snake\"\n",
    "* When and Why can there be a statistical advantage from using a distributed representation as part of a learning algorithm?\n",
    "  * When\n",
    "    * complicated structure can be compactly represented using a small number of parameters (dim vector size)\n",
    "    * Bigger Dim of parameters -> larger degree of freedom -> larger regions -> larger data\n",
    "  * Why\n",
    "     * The Number of Distinguishable Regions using linear threshold units\n",
    "     * $d:= \\text{Input Dim}$, $n:= \\text{Feature Value Dim}$\n",
    "       * $\\Sigma_{j=0}^{d} \\binom{n}{j}= O(n^{d})$ using only $O(nd)$\n",
    "     * Can extended to the case using nonlinear units\n",
    "       * represent Larger regions using Smaller parameter\n",
    "       * fewer example to generalize well\n",
    "     * Effective in Capacity\n",
    "       * If $w$ is number of weights, VC Dimension is $O(w\\mathbf{log}w)$,  \n",
    "         * VC Dimension(Vapnik-Chervonenkis)?\n",
    "            * VC of One Linear classifier is 3\n",
    "  <img src=http://2.bp.blogspot.com/-wtSDb_rZs4M/Vo8FaQzrbmI/AAAAAAAADUE/kv4sQAuuzJc/s640/line%2Bshatter.png width=400>\n",
    "  \n",
    "    * Learning about each of them without having to see all the configurations of all the others\n",
    "      * e.g. If we learn about man with glasses, man without glasses and woman without glasses\n",
    "        * we can infer woman with glasses.\n",
    "<img src=\"Figure_15_9.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.5 Exponential Gains from Depth\n",
    " \n",
    "* Functions can be represented by exponentially smaller deep networks compared to shallow networks\n",
    "  * Small Deep networks can represents better than shallow networks\n",
    "  \n",
    "* e.g. Generative Model\n",
    "  * Need highly nonlinear ways to the input in order to generate data\n",
    "  * High nonlinearity\n",
    "    * Composition of many nonlinearities and a hierarchy of reused features can give an exponential boost to statistical efficiency, on top of the exponential boost given by using a Distributed Representation\n",
    "    * Deep Network + Distributed Representation => High nonlinearity\n",
    "    * e.g. Simple Universal Approximator (Ch 6)\n",
    "      * Boolean gates, sum/products, RBF with even a single hidden layer\n",
    "      * Can approximate a large class of functions\n",
    "      * Expressive Power\n",
    "         * Need \"exponential\" number of hidden units in order to have same expressive power of architecture with additional 1 depth.\n",
    "    * Similar Result on...\n",
    "      * Deterministic fead forward network as a universal approximators of \"Probability Distribution\"\n",
    "        * Many structured probabilistic models with a single hidden layer of latert variables (Ch 16)\n",
    "        * e.g. Boltzman machines, Deep Belief Networks\n",
    "        * Deeper one can have \"exponential\" advantage over a shallow one.\n",
    "      * sum-product network for probabilistic models (SPN)\n",
    "      <img src=http://web.engr.oregonstate.edu/~sinisa/images/research/SPN.png width=200>\n",
    "      * Deep circuits related to convolutional networks (Convolutional sum-product network)\n",
    "      <img src=\"Convolutional_Arithmetic_Circuit.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.6 Providing Clues to Discover Underlying Causes\n",
    "\n",
    "\n",
    "* What makes one representation better than another?\n",
    "  * One that disentangles the underlying causal factors\n",
    "  * Learner separate these observed factors from the others\n",
    "  * Introduce clues that help the learning to find these underlying factors from the others.\n",
    "  * Type 1: Supervised Learning\n",
    "    * Provides a very strong clue a label $\\mathbf{y}$\n",
    "  * Type 2: Using of abundant Unlabeled data\n",
    "    * hints about the underlying factors\n",
    "      * take the form of implicit prior beliefs that the designers of the learning algorithm impose in order to guide the learner.\n",
    "      * Regularization strategies are necessary to obtain good generalization\n",
    "      * One goal of deep learning is to find a set of fairly generic regularization strategies.\n",
    "      \n",
    "* Generic Regularization Stretegies\n",
    "  * Smoothness\n",
    "    * $f(x + \\epsilon d) \\approx f(x)$ for unit $d$\n",
    "    * allow to generalize from training examples to nearby points in input space\n",
    "  * Linearity\n",
    "    * Relationships btw some variables are linear.\n",
    "    * Make predictions even very far from the observed data,\n",
    "      * sometimes lead to overrly extreme prediction\n",
    "    * Simple machine learning algorithms use \"Linearity\" instead of \"Smoothness\"\n",
    "      * Linearity and Smoothness are different assumption in \"High Dimensional\" space\n",
    "  * Multiple Explanatory Factors (Output)\n",
    "    * Motivation $p(x) \\approx p(y|x)$ in Semi-Supervised Learning\n",
    "    * Distributed Representation\n",
    "  * Causal Factors (Input)\n",
    "    * Underlying Causal Factor $h$ in Semi-Supervised Learning\n",
    "  * Depth or a Hierarchical Organization of Explanatory Factors\n",
    "    * High level can be defined in terms of simple concepts forming a hierarchy.\n",
    "      * Cat (High level), pointy ears, 4 legs (Lower level)\n",
    "    * Multi step program\n",
    "      * Each step (layer) refer back to the output via previous step (layer)\n",
    "  * Shared Factors across Tasks\n",
    "    * In Many tasks: differernt $\\mathbf{y_i}$ outputs sharing $\\mathbf{x}$ input.\n",
    "      * There are $f^{i}(\\mathbf{x})$ of a global $\\mathbf{x}$\n",
    "      * Each $\\mathbf{y_i}$ is associated with a different subset from $\\mathbf{h}$\n",
    "        * $P(\\mathbf{y_i} | \\mathbf{x})$ depends on  $P(\\mathbf{h} | \\mathbf{x})$\n",
    "  * Manifolds\n",
    "    * Regions in which probability mass concentrates are...\n",
    "      * locally connected\n",
    "      * occupy a tiny volume\n",
    "    * These regions can be approximaed by Low-dimensional manifolds with a much smaller Dim\n",
    "    * Motivate Autoencoders\n",
    "  * Natural Clustering\n",
    "    * Each manifold in the input space may be assigned to a single class.\n",
    "    * The data may lie on many disconnected manifolds\n",
    "    * Motivate tangent propagation, double backprop, manifold tangent classifier, adversarial training\n",
    "  * Temporal and Spatial Coherence\n",
    "    * Most important explanatory factors change slowly over time\n",
    "  * Sparsity\n",
    "    * Most features should presumabley not be relevant to describing most inputs\n",
    "  * Simplicity of Factor Dependencies\n",
    "    * The simplest example\n",
    "      * $P(\\mathbf{h}) = \\Pi_i(\\mathbf{h_i})$\n",
    "    * Linear dependencies, dependencies captured by a autoencoder\n",
    "    * Motivate many laws of physics\n",
    "    * Motivate a linear predictor or a factorized prior on top of a learned representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
