{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contents\n",
    "* Reviews\n",
    "* The Log-Likelihood Gradient\n",
    "* Stochastic Maximum Likelihood and Contrastive Divergence\n",
    "* Pseudolikelihood\n",
    "* Score Matching and Ratio Matching\n",
    "* Denoising Score Matching\n",
    "* Noise-Contrastive Estimation\n",
    "* Estimating the Partition Function\n",
    "  * Annealed Importance Sampling\n",
    "  * Bridge Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews\n",
    "\n",
    "* Probability vs Compatibility\n",
    "  * $p(x) = \\cfrac{1}{Z}\\tilde{p}(x)$\n",
    "  * where $Z = \\int_x \\tilde{p}(x)$\n",
    "  * $Z$ is Partition Function \n",
    "* Energy-based model\n",
    "<img  src=\"SRLTQV05FB5UBWRN9UDSJELJ1DRUI1FC.png\" width=600/>\n",
    "  * $\\tilde{p}(\\mathbf{x}) = \\exp(-E(\\mathbf{x}))$\n",
    "    * to Make $\\forall{\\mathbf{x}},\\tilde{p}(\\mathbf{x}) > 0$\n",
    "    * $E(\\mathbf{x})$ is Energy Function\n",
    "    * $\\tilde{p}(\\mathbf{x})$ in EBM is an example of a Boltzmann distribution.\n",
    "\n",
    "* MCMC\n",
    "  * We can do Sampling from System on Steady State even though random initializatoin\n",
    "  * Approximation for Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.1 Log-likelihood Gradient\n",
    "\n",
    "* Gradient\n",
    "  * <img  src=\"PJ5U2NA13VAVD6U2LJOYR8B2534R9VFR.png\" width= 400/>\n",
    "* Partition Z\n",
    "  * <img  src=\"LMVG828BPQNWEWC9KXUH3V3MJQR23WNK.png\" width=600/>\n",
    "  * <img  src=\"CLGAS72B313BQOLV3OD06NL0A8CAAOKR.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.2 Stochastic Maximum Likelihood and Contrastive Divergence\n",
    "\n",
    "* How to use gibbs_update (sampling conditionally and partially using MCMC)\n",
    "* How to make it Faster\n",
    "* Positive Phase vs Negative Phase\n",
    "  * $\\nabla_\\theta \\log p(x)=\\nabla_\\theta \\log \\tilde{p}(x) - \\mathbb{E}_{x\\sim p(x)} [\\nabla_\\theta \\log \\tilde{p}(x)]$\n",
    "    * Positive: $\\nabla_\\theta \\log \\tilde{p}(x)$\n",
    "    * Negative: $\\mathbb{E}_{x\\sim p(x)} [\\nabla_\\theta \\log \\tilde{p}(x)]$\n",
    "  * Practical Calculation\n",
    "    * Gradient update $=\\Sigma_{x\\sim data} \\nabla_\\theta \\log \\tilde{p}(x) - \\Sigma_{x\\sim MCMC} \\nabla_\\theta \\log \\tilde{p}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"YBQ30HJSELRUVFPCSE9OXC6SA4TP3PA7.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Algorithm \n",
    "<img  src=\"H21Y4NO2ADFIO7MN7DRKRTSC93GNY52I.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Divergence Algorithm (CD)\n",
    "\n",
    "* Motivation\n",
    "  * Iteration step for gibbs_update, k=100 is too expensive computationally.\n",
    "  * Reduce k into 1-20 for model on a small image patch\n",
    "<img  src=\"R90F96NWSLVOTN1948YETJSVXXBX57X6.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"W1EWXN4R96DR6C2K6RQD1I9FT1G9NOCR.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Maximum Likelihood Algorithm (SML)\n",
    "\n",
    "* Also called Persistant Contrastive Divergence (PCD)\n",
    "* Reduce k into 1 for model on a small image patch\n",
    "* How\n",
    "  * Maintain contrastive samples persistantly!!! (So Simple)\n",
    "<img  src=\"Y9JCHDQI6LH0XOKFNVYRVY5PW3RNPSBG.png\" width=700/>\n",
    "\n",
    "* Theree are additional approach to accelerate SML called as Faster PCD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.3 Pseudolikelihood\n",
    "\n",
    "* Set $\\log p(\\vec{x}) \\approx \\sum_d \\log p(x_d|\\vec{x}_{-d})$\n",
    "* Use Conditional Probability property\n",
    "  * $p(x_1|x_2)=\\cfrac{p(x_1,x_2)}{p(x_2)}=\\cfrac{\\tilde{p}(x_1,x_2)}{\\tilde{p}(x_2)}=\\cfrac{\\tilde{p}(x_1,x_2)}{\\int_{x_1}\\tilde{p}(x_1,x_2)}$\n",
    "* e.g.\n",
    "  * If we want to calculate $p([1,1,1,1])$ where each state = 1~3.\n",
    "    * We have to evaluate $p(x_1=1|x_2=1, x_3=1, x_4=1])$\n",
    "      * By calculating model $\\tilde{p}$\n",
    "        * $\\tilde{p}(x_1=1||x_2=1, x_3=1, x_4=1)$\n",
    "        * $\\tilde{p}(x_1=2||x_2=1, x_3=1, x_4=1)$\n",
    "        * $\\tilde{p}(x_1=3||x_2=1, x_3=1, x_4=1)$\n",
    "      * $p(x_1=1|x_2=1, x_3=1, x_4=1]) = \\cfrac{\\tilde{p}(x_1=1||x_2=1, x_3=1, x_4=1)}{\\sum_k \\tilde{p}(x_1=k||x_2=1, x_3=1, x_4=1)}$\n",
    "    * iterate D step\n",
    "    * Finally\n",
    "      * $p(x_1=1|[1, 1, 1]) \\times p(x_2=1|[1, 1, 1]) \\times p(x_3=1|[1, 1, 1]) \\times p(x_4=1|[1, 1, 1])$\n",
    "  * $K^D$ -> $K\\times D$  \n",
    "* This may look like an unprincipled hack, but it can be proven that estimation by maximizing the pseudolikelihood is asymptotically consistent\n",
    "* \"Generalized\" Pseudolikelihood\n",
    "  * Make dimensions into $m$ Partitions\n",
    "    * If $m=1$\n",
    "      * No partition\n",
    "      * Exact likelihood\n",
    "    * If $m=D$\n",
    "      * Partition for a individual dimension\n",
    "      * Same as Pseudolikelihood\n",
    "    * If $1<m<D$\n",
    "      * $\\log p(x) \\approx \\sum_m p(x_m|\\vec{x}_{-m})$\n",
    "<img  src=\"OHIU5CVP5SD67HQM6ESBBI0TDKS2W7S1.png\"/>\n",
    "\n",
    "* It can perform better than maximum likelihood for \"tasks that require only the conditional distributions used during training\"\n",
    "  * such as filling in small amounts of missing values.\n",
    "\n",
    "\n",
    "* Generalized pseudolikelihood techniques are especially powerful\n",
    "  * If designed to capture the most important correlations\n",
    "    * For example, in natural images, pixels that are widely separated in space also have weak correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.4 Score Matching and Ratio Matching\n",
    "\n",
    "## Score Matching\n",
    "\n",
    "* Like Pseudolikelihood, Score Matching does not approximate Partition $Z$\n",
    "* Score\n",
    "  * $\\nabla_x \\log p(x)$\n",
    "<img  src=\"VLA45G8R0SY3XCMM1PL56VM7MD9N3N0Q.png\" width=600/>\n",
    "* Approximation\n",
    "\n",
    "<img  src=\"NR7TGUVPMVQ17W8F87PQ058HQJUB5UK8.png\" width=600/>\n",
    "## Ratio Matching\n",
    "\n",
    "* A more successful approach to extending the basic ideas of score matching to discrete data \n",
    "* Ratio?\n",
    "<img  src=\"VIRU0L40JP9E43MKIKBHN9M30WY72EGG.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
