{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* Reviews\n",
    "* Introduction\n",
    "* Inference as Optimization\n",
    "* Expectation Maximization\n",
    "* MAP Inference and Sparse Coding\n",
    "* Variational Inference and Learning\n",
    "  * Discrete Latent Variables\n",
    "  * Calculus of Variations\n",
    "  * Continuous Latent Variables\n",
    "  * Interactions between Learning and Inference\n",
    "* Learned Approximate Inference\n",
    "  * Wake-Sleep\n",
    "  * Other Forms of Learned Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews\n",
    "\n",
    "## Restricted  Boltzmann Machine (Ch 16)\n",
    "<img  src=\"DR23XARKAK1OJG5MDJETKLSIW8PEOAG6.png\"/>\n",
    "\n",
    "* Energy-based Model\n",
    "  * $\\tilde{p}(\\mathbf{x})=\\exp(-E(\\mathbf{x}))$\n",
    "\n",
    "<img  src=\"S1X0KQP97T23G5QX6L7G2QIH3VIBD0QD.png\"/>\n",
    "\n",
    "<img  src=\"NAK9BFBQ7RDDAB9RD1IOICDMFALQX3BP.png\"/>\n",
    "\n",
    "<img  src=\"S1YQFFW9S57PAD8W52R6FFTMEMIBFSAJ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"P7KADC64VURK2IVABTPF27B6H2241WQW.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"O0C3C7YKQ34BDB0PSNRHKC6VREILBW4V.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Case of RBM\n",
    "\n",
    "* All $\\mathbf{h}_i$, $\\mathbf{v}_i$ are 0 or 1\n",
    "<img  src=\"Q4UR9PGI9NIXPK8LQX7OIKHT8BW3WBL8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"KA6O2NPDWROIC1P0T30AXUDANVDM3DAE.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can extract closed form $p(\\mathbf{h}|\\mathbf{v})$, $p(\\mathbf{v}|\\mathbf{h})$\n",
    "* How $p(\\mathbf{v})$?\n",
    "<img  src=\"WE990XBY9VDGTW5EB0441NMIY555U29D.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"XLUK9NTJAVBFV63WW4BQ0D8HFF5C57EN.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intractability of computing Partition Functions, Again\n",
    "\n",
    "* Computing $\\tilde{p}(\\mathbf{v})$ is easy\n",
    "* But How to compute $p(\\mathbf{v}) = \\cfrac{1}{Z} \\tilde{p}(\\mathbf{v})$\n",
    "* We need to Compute Partition Function $Z$\n",
    "<img  src=\"O0C3C7YKQ34BDB0PSNRHKC6VREILBW4V.png\"/>\n",
    "\n",
    "* Given Visible\n",
    "  * $\\mathbf{x}^{(i)}$\n",
    "* Generated \"Visible\" and Hidden Variables from Gibbs Sampling\n",
    "  * $\\tilde{\\mathbf{x}}^{(i)}$, $\\mathbf{h}^{(j)}$\n",
    "* Variables: $W$, $\\mathbf{b}$, $\\mathbf{c}$\n",
    "  * Calculating Gradients of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"C87MXUD6E3NYCBAQ3V4L8T70RYNNQLJE.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "> The Chanllege of inference usually refers to the difficult problem of computing $p(h|v)$ or taking expectations with respect to it\n",
    "\n",
    "* $p(v)$, $p(h|v)$, $p(v|h)$ are important! for the next explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"SE4APQUGKS87K7VML2EPNVW8TMKFM735.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.1 Inference as Optimization\n",
    "\n",
    "\n",
    "* What we want to know $\\log p(v;\\theta)$\n",
    "  * Inference of $\\log p(v;\\theta)$ is intractable\n",
    "  * We get Lower Bound $L$ instead of it\n",
    "  * Consider hidden variables $h$\n",
    "  * Induce new probability $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"TGQX8E77RM8PGYRC475K79AX3RS6OLRC.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"LX0G7OB9WY139T80BV7CND659XJ9YFN1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L$ is tighter\n",
    "  * => $q(h|v)$ are better approximations of $p(h|v)$\n",
    "  * $q(h|v) == p(h|v)$ => $L = \\log p(v;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.2 Expectation Maximization\n",
    "\n",
    "* A Popular Training Algorithm for models with Latent Variables.\n",
    "  * K-Mean\n",
    "* Not Approximate Inference\n",
    "* Approximate Posterior\n",
    "* Stochastic gradient ascent on latent variable models can be seen as a special case of the EM algorithm\n",
    "  * where the M step consists of taking a single gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"R9VVIJ50IY3DX4PS2OLILCJ0L78Y8GKY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"CSP1XGHWC3SQB2RGNJWNHFDD6EDCPYCT.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.3 MAP Inference and Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.4 Variational Inference and Learning\n",
    "\n",
    "* \"VARIATIONAL\"\n",
    "  * Use function as parameters\n",
    "  * function $q$ in $\\mathcal{L}(\\mathbf{v}, \\theta, q)$\n",
    "  \n",
    "* The Core Idea\n",
    "  * Maximize $\\mathcal{L}$ over a restricted family of distributions q.\n",
    "    * Not just control $\\theta$\n",
    "    * Control a family of $q$\n",
    "  * Impose the restriction on $q$\n",
    "\n",
    "* Mean field Approach\n",
    "  * $q(\\mathbf{h}|\\mathbf{v}) = \\Pi_{i} q(\\mathbf{h}_i|\\mathbf{v})$\n",
    "  * to maximize $\\log p(\\mathbf{v};\\theta)-D_{KL}(q(\\mathbf{h}|\\mathbf{v})||p(\\mathbf{h}|\\mathbf{v};\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"G01K67ROUJLBNVN715PPRWN0GTQKJOQT.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 19.4.1 Discrete Latent Variables\n",
    "\n",
    "### Binary sparce coding model case\n",
    "* $\\mathbf{h}_i$ is binary.\n",
    "* set $\\hat{h}_i = q(\\mathbf{h}_i=1|\\mathbf{v})$\n",
    "  * $1 - \\hat{h}_i = q(\\mathbf{h}_i=0|\\mathbf{v})$\n",
    "* $p(h_i = 1) = \\sigma(b_i)$\n",
    "* $p(v|h) = \\mathcal{N}(v;Wh, \\beta^{-1})$\n",
    "\n",
    "* Target\n",
    "  * $p(v)$ Only!!!\n",
    "  * Use $h$\n",
    "    * generated by $\\sigma(b_i)$\n",
    "    * $b_i$ is key!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"IOTH3NVWEJXXBQJN0V9GSMJB1NW59CU4.png\" width=300px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find $b_i$ to maximize $p(v)$\n",
    "  * We need $p(h|v)$ \n",
    "<img  src=\"CU489X3VR4UW3477UWJG06TBD0O9DFAG.png\"/>\n",
    "<img  src=\"IUP8HPW8TOB62AHMMTF3EHDJWP3HKLDV.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $p(h|v) \\approx q(h|v) = \\Pi_i q(h_i|v)$\n",
    "  * Remove $p(h|v)$ and insert $\\Pi_i q(h_i|v)$\n",
    "  * set $\\hat{h}_i = q(\\mathbf{h}_i=1|\\mathbf{v})$\n",
    "  * set $1 - \\hat{h}_i = q(\\mathbf{h}_i=0|\\mathbf{v})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"QMVGLTRN1IYEOLJJ63GL19EJWV8UEU31.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"SHUIT9GV1MMMDKLRW71JK6H5FMB1RK2Q.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"UX8JN9XE3TG4HEMUW27GKVXNOYX52CE2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"MB2MK7X5GS9AWY8D90W6UJRGA8QHN86O.png\"/ width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fixed point update, equation\n",
    "<img  src=\"SSSEH9CEM0KXE8MAGS3YCMXVYGNSXSE7.png\"/>\n",
    "  * $\\hat{h}_i^t = f(\\hat{h}_j^{t-1})$\n",
    "  * $\\hat{h}_j^t = f(\\hat{h}_i^{t-1})$\n",
    "  * After iterations, the values converge true-answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "  * Calculate $\\hat{h}_i$ using other $\\hat{h}_j$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.4.2 Calculus of Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.4.3 Continuous Latent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"NLS0HUAMLAT2RB4LP37YRLTLQTR5HXI6.png\"/>\n",
    "<img  src=\"WBGA3PTX7XENI88NBFTUHEN0U51G9SXW.png\"/>\n",
    "* This $\\tilde{q}$s maximize $\\mathcal{L}(v,\\theta, q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Induction based on Kevin Murphy's Book\n",
    "> <img  src=\"YD23DTOU2XWKHEW9YO78SN0WDG6T30V4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assumption for simplicity\n",
    "  * $h \\in  \\mathbb{R}^2$ -> $i = 1,2$\n",
    "  * $p(h) = \\mathcal{N}(h;0, \\mathbf{I})$\n",
    "  * $p(v|h) = \\mathcal{N}(v;W^Th,1)$\n",
    "* Find Compatibility, $\\tilde{p}$ (Unnormalized Probability)\n",
    "<img  src=\"VQD00WY6161B8WADN0YIKWULS8K35G41.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"UBXYD4QMW1OLVGQSK8EYIYUT8399ASU1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reduce Notations\n",
    "  * $\\langle h_2 \\rangle=\\mathbb{E}_{h_2 \\sim q(h|v)}[h_2]$\n",
    "  * $\\langle h^2_2 \\rangle=\\mathbb{E}_{h_2 \\sim q(h|v)}[h^2_2]$\n",
    "<img  src=\"EPV20G9IYRGICK9LCYVEKFMSY2ALSFNS.png\"/>\n",
    "\n",
    "\n",
    "\n",
    "* We have proved $\\tilde q$ is Gaussian, $q$ is also Gaussian.\n",
    "  * set new $q=\\mathcal{N}(h;\\mu, \\beta^{-1})$\n",
    "  * Find $\\mu, \\beta$ by using traditional oprimization method\n",
    "  * $\\mu, \\beta$ are parameters for variational approximate\n",
    "  * $w$ is a parameter for learning process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Process\n",
    "> main-loop for gradient update\n",
    ">>1. approximate inference-loop\n",
    ">>>update $q_i$s\n",
    ">>2. loop for-MCMC for partition function\n",
    ">>>sampling\n",
    ">>3. update gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.4.4 Interactions between Learning and Inference\n",
    "* Approximate inference <-> Learning process\n",
    "* Final Goal is to maximize $p(v,h)$\n",
    "* Intermediate Goal is to maxmize $\\mathbb{E}_{h\\sim q} \\log p(v,h)$\n",
    "* Modality difference can cause bad approximate\n",
    "* Need to calculate the difference between $\\log p(v;\\theta)$ and $\\mathcal{L}(v, \\theta, q)$ for checking approximate quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.5 Learned Approximate Inference\n",
    "\n",
    "* Optimization via iterative procedures such as fixed-point equations is often very expensive and time-consuming.\n",
    "* find inference network $\\hat{f}(v;\\theta) \\approx q$ \n",
    "\n",
    "## 19.5.1 Wake-Sleep\n",
    "* awake -> update $\\theta$\n",
    "* asleep -> update $\\hat{f}$\n",
    "> main-loop\n",
    ">> 1. wake-loop\n",
    ">>>update $\\hat{f}$\n",
    ">> 2. sleep-loop\n",
    ">>>update gradient\n",
    "* c.f. mean-field approximation loop\n",
    "> main-loop for gradient update\n",
    ">>1. approximate inference-loop\n",
    ">>>update $q_i$s\n",
    ">>2. loop for-MCMC for partition function\n",
    ">>>sampling\n",
    ">>3. update gradient\n",
    "\n",
    "## 19.5.2 Other Forms of Learned Inference\n",
    "* Learned approximate inference has recently become one of the dominant approaches to generative modeling\n",
    "  * In the form of the variational autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
