{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* The Challenge of Unstructured Modeling \n",
    "* Using Graphs to Describe Model Structure\n",
    "  * Directed Models\n",
    "  * Undirected Models\n",
    "  * The Partition Function\n",
    "  * Energy-Based Models\n",
    "  * Separation and D-Separation\n",
    "  * Converting between Undirected and Directed Graphs\n",
    "  * Factor Graphs\n",
    "* Sampling from Graphical Models (Generating Sample using estimated distribution)\n",
    "* Advantages of Structured Modeling\n",
    "* Learning about Dependencies\n",
    "* Inference and Approximate Inference\n",
    "* The Deep Learning Approach to Structured Probabilistic Models (IMPORTANT!!!)\n",
    "  * Example: Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* A Structured probabilistic model is a way of describing a probability distribution,using a graph to describe which random variables in the probability distribution interact with each other directly.\n",
    "  * Graph: Vretices, Edges\n",
    "* Deep learning Practitionors tend to use very different model structures, learning algorithms and inference procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.1 The Challenge of Unstructured Modeling\n",
    "* Tasks using Probabilistic Models are often more expensive than classification.\n",
    "  * Multiple Output Values\n",
    "  * c.f. Classification produces a single output, and ignores many parts of the input\n",
    "* Probabilistic Models => a complete understanding of the entire structure of the input\n",
    "  * Density Estimation\n",
    "    * Base-\n",
    "  * Denoising\n",
    "    * Robust to noise\n",
    "  * Missing value imputation\n",
    "    * Impute a Probability over unobserved data\n",
    "  * Sampling\n",
    "    * Generates new samples from the distribution\n",
    "* Requirements for the models\n",
    "  * Memory for representation\n",
    "    * c.f. Naive Distribution: n dimension with k states for each entry => $O(k^n)$ \n",
    "  * Statistical Efficiency\n",
    "    * the number of model parameters $\\uparrow$, the required amount of training data $\\uparrow$\n",
    "  * The cost of inference\n",
    "  * The cost of sampling\n",
    "* Structured probabilistic models provide a formal framework for modeling only direct interactions between random variables.\n",
    "  * Fewer parameters\n",
    "  * Reduce computational cost\n",
    "    * storing the model, inference, sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.2 Using Graphs to Describe Model Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.1 Directed Models\n",
    "\n",
    "\n",
    "\n",
    "* Known as Belief Network, Bayesian Network\n",
    "* Directed Edge means conditional distribution\n",
    "  * e.g. relay run $p(t_0, t_1, t_2) = p(t_0)p(t_1|t_0)p(t_2|t_1)$\n",
    "    <img src=\"Figure_16_2.png\" width=300>\n",
    "* General Case\n",
    "  * $p(\\mathbf{x}) = \\Pi_i p(x_i| Pa_\\mathcal{G}(x_i))$\n",
    "    * $Pa_\\mathcal{G}(x_i)$ means 'Pa'rent nodes of $x_i$ in $\\mathcal{G}$.\n",
    "  \n",
    "* n discrete variables each having k values\n",
    "  * Unstructured Models: $O(k^n)$ parameters\n",
    "  * Structured Models: $O(k^m)$ parameters (m is the maximum number of variables in a single conditional p(...).\n",
    "    * few parents in the graph -> few paramters. -> efficient\n",
    "* Graph encoding\n",
    "  * \"Which variables are Conditionally independent from each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.2 Undirected Models\n",
    "\n",
    "* Known as Markov Random Fields(MRFs), Markov Networks\n",
    "* Use when the interactions seem to have no intrinsic direction or to operate in both directions.\n",
    "  * e.g. Health of your roommate, you, your coworker\n",
    "<img src=\"Figure_16_3.png\" width=300>\n",
    "  \n",
    "* General Case\n",
    "  * Unnormalized Probability Distribution for Undirected Models\n",
    "    * $\\tilde{p}(\\mathbf{x})=\\Pi_{C\\in \\mathcal{G}} \\phi(C)$\n",
    "      * C is clique: a subset of nodes connected to each other.\n",
    "      * $\\phi(C)$ is clique potential\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.3 The Partition Function\n",
    "\n",
    "* Normarlized Probability Distribution for Undirected Models\n",
    "  * $p(x) = \\frac{1}{Z}\\tilde{p}(\\mathbf x)$\n",
    "    * $Z = \\int \\tilde{p}(\\mathbf x) dx$\n",
    "* $Z$ is known as the partition function, a term borrowed from statistical physics.\n",
    "  * often intractable to compute.\n",
    "  * $phi$ must be conducive to computing $Z$ efficiently.\n",
    "  * We need Approximations (CH 18)\n",
    "  \n",
    "* Difference btw Directed and Undirected Models\n",
    "```\n",
    " Directed models are defined directly in terms of probaility distributions from the start, while undirected models are defined more loosely by $\\phi$ functions that are then converted into probability distributions.\n",
    "```\n",
    "* The domain of each of the variables has dramatic effect on the kind of probability distribution that a given set of $\\phi$ corresponds to.\n",
    "* Often, it is possible to elverage the effect of a carefully chosen domain of a variable in order to obtain complicated behavior from a relatively simple set of $\\phi$.\n",
    "  * e.g. Z diverge or not? $\\mathbf{x} \\in \\mathbb{R}^n or \\{ 0,1 \\}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.4 Energy-Based Models (EBM)\n",
    "\n",
    "* $\\tilde{p}(\\mathbf{x}) = \\exp(-E(\\mathbf{x}))$\n",
    "  * to Make $\\forall{\\mathbf{x}},\\tilde{p}(\\mathbf{x}) > 0$\n",
    "  * $E(\\mathbf{x})$ is Energy Function\n",
    "  * $\\tilde{p}(\\mathbf{x})$ in EBM is an example of a Boltzmann distribution.\n",
    "    * Many energy-based models are called Boltzmann machines.\n",
    "* Different cliques in the undirected graph correspond to the different terms of the energy function.\n",
    "  <img src=\"Figure_16_4.png\" width=600>\n",
    "  <img src=\"Figure_16_5.png\" width=600>\n",
    "  * a EBM is just a special kind of Markov network\n",
    "  * One can view a EBM a Product of \"Experts\"(each term)\n",
    "* The words such as \"Partition function\", \"Energy\" are originally developed by statistical physicists.\n",
    "* Not compute $p_\\text{model}(\\mathbf{x})$ but only $\\log \\tilde{p_\\text{model}}(\\mathbf{x})$\n",
    "  * Free energy with latent variables $\\mathbf{h}$\n",
    "    * $\\mathcal{F} = -\\log \\Sigma_h \\exp(-E(\\mathbf{x}, \\mathbf{h}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.5 Separation and D-Separation\n",
    "* Which variables indirectly interact?\n",
    "* Separation (for Undirected Models)\n",
    "  * Conditionally independence implied by the graph\n",
    "  <img src=\"Figure_16_6.png\" width=600>\n",
    "  * If (1) No path exists between them or (2) all paths contain an observed variable, => Separated\n",
    "  * Active/Inactive\n",
    "    * paths involving only unobserved variables => Active\n",
    "    * all paths including an observed variable => Inactive\n",
    "  <img src=\"Figure_16_7.png\" width=600> \n",
    "* d-Separation (for Directed Models)\n",
    "  * \"d-\" means \"dependence\" \n",
    "  * e.g.\n",
    "  <img src=\"Figure_16_8.png\" width=600>\n",
    "  * e.g.\n",
    "  <img src=\"Figure_16_9.png\" width=600>\n",
    "\n",
    "* Context-specific Independences\n",
    "  * Cannot be represented with Existing Graphical Models\n",
    "  * Independences that are present dependent on the value of some variables in the network.\n",
    "  * e.g.\n",
    "    1. a = 0 => b and c are independent\n",
    "    2. a = 1 => b = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.6 Converting between Undirected and Directed Graphs\n",
    "\n",
    "* We often refer to a specific machine learning model as being undirected or directed.\n",
    "  * RBM: undirected\n",
    "  * Sparse coding: Directed\n",
    "* But Every probability distribution can be represented by either a Directed or an Undirected\n",
    "  * No probabilistic model is inherently directed or undirected.\n",
    "  * Some models are most easily described using a directed (or undirected) graph\n",
    "* We should \"Choose\" which language to use for each task.\n",
    "  * Which approach can capture the most independences.\n",
    "  * Which approach uses the fewest edges\n",
    "* We may sometimes switch between different modeling languages fwith a single probability distribution\n",
    "  * Sampling: Directed (Ch16.3)\n",
    "  * Approximate Inference: Undirect (Ch19)\n",
    "* Converting Directed into Undirected\n",
    "  * Immorality, Moralized Graph\n",
    "  <img src=\"Figure_16_11.png\" width=600>\n",
    "* Converting Undirected into Directed\n",
    "  * Chord\n",
    "    * A connection between any two non-consecutive variables in the loop.\n",
    "  * Converting to Chordal or Triangulated Graph\n",
    "    * Remove loops of length greater than 3.\n",
    "    * Make all loops as Triangular loops\n",
    "  * WHY?\n",
    "    * Review \"Separated in Undirected\" and \"d-Separated in Directed\"\n",
    "  <img src=\"Figure_16_12.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.7 Factor Graphs\n",
    "\n",
    "* Another way of drawing Undirected Models\n",
    "  * Resolve an ambiguity in the graphical representation\n",
    "  * Factor graphcs explicitly represent the scope of each $\\phi$.\n",
    "  <img src=\"Figure_16_13.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 Sampling from Graphical Models\n",
    "\n",
    "* Ancestral Sampling\n",
    "  * Use \"ONLY\" Directed Graphical Models\n",
    "  * Sort variables $\\mathbf{x}_i$ in the graph into a topological ordering.\n",
    "  * Sample in this order\n",
    "  * e.g. If $\\mathbf{x}_1$ -> $\\mathbf{x}_2$ -> $\\mathbf{x}_3$ ... -> $\\mathbf{x}_n$\n",
    "    * Sample $\\mathbf{x}_1 \\sim P(\\mathbf{x}_1)$\n",
    "    * Sample $\\mathbf{x}_2 \\sim P(\\mathbf{x}_2 | \\mathbf{x}_1)$\n",
    "    * ...\n",
    "    * Sample $\\mathbf{x}_n \\sim P(\\mathbf{x}_n | \\mathbf{x}_{n-1})$\n",
    "* Ancestral Sampling for Undirected Models\n",
    "  * Preprocessing: Converting Undirected to Directed\n",
    "  * Some undirected Models cannot be converted into Directed\n",
    "  * Some converted Models becomes intractable.\n",
    "* Gibb Sampling for Undirected Models (Ch17)\n",
    "  * Use separation properties\n",
    "  * Iteratively visit each vaiable $\\mathbf{x}_i$\n",
    "  * Not a fair sample $\\mathbf{x} \\sim P(\\mathbf{x})$\n",
    "  * Focus only one variable and \"Local\" condition on only the neighbors sampling\n",
    "  * Repeat!!! -> Converges to sampling from the correct distribution\n",
    "  * But When we stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.4 Advantages of Structured Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.5 Learning about Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.6 Inference and Approximate Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.7 The Deep Learning Approach to Structured Probabilistic Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7.1 Example: The Restricted Boltzmann Machine "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
