{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* The Challenge of Unstructured Modeling \n",
    "* Using Graphs to Describe Model Structure\n",
    "  * Directed Models\n",
    "  * Undirected Models\n",
    "  * The Partition Function\n",
    "  * Energy-Based Models\n",
    "  * Separation and D-Separation\n",
    "  * Converting between Undirected and Directed Graphs\n",
    "  * Factor Graphs\n",
    "* Sampling from Graphical Models (Generating Sample using estimated distribution)\n",
    "* Advantages of Structured Modeling\n",
    "* Learning about Dependencies\n",
    "* Inference and Approximate Inference\n",
    "* The Deep Learning Approach to Structured Probabilistic Models (IMPORTANT!!!)\n",
    "  * Example: Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* A Structured probabilistic model is a way of describing a probability distribution,using a graph to describe which random variables in the probability distribution interact with each other directly.\n",
    "  * Graph: Vretices, Edges\n",
    "* Deep learning Practitionors tend to use very different model structures, learning algorithms and inference procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.1 The Challenge of Unstructured Modeling\n",
    "* Tasks using Probabilistic Models are often more expensive than classification.\n",
    "  * Multiple Output Values\n",
    "  * c.f. Classification produces a single output, and ignores many parts of the input\n",
    "* Probabilistic Models => a complete understanding of the entire structure of the input\n",
    "  * Density Estimation\n",
    "    * Base-\n",
    "  * Denoising\n",
    "    * Robust to noise\n",
    "  * Missing value imputation\n",
    "    * Impute a Probability over unobserved data\n",
    "  * Sampling\n",
    "    * Generates new samples from the distribution\n",
    "* Requirements for the models\n",
    "  * Memory for representation\n",
    "    * c.f. Naive Distribution: n dimension with k states for each entry => $O(k^n)$ \n",
    "  * Statistical Efficiency\n",
    "    * the number of model parameters $\\uparrow$, the required amount of training data $\\uparrow$\n",
    "  * The cost of inference\n",
    "  * The cost of sampling\n",
    "* Structured probabilistic models provide a formal framework for modeling only direct interactions between random variables.\n",
    "  * Fewer parameters\n",
    "  * Reduce computational cost\n",
    "    * storing the model, inference, sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.2 Using Graphs to Describe Model Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.1 Directed Models\n",
    "\n",
    "\n",
    "\n",
    "* Known as Belief Network, Bayesian Network\n",
    "* Directed Edge means conditional distribution\n",
    "  * e.g. relay run $p(t_0, t_1, t_2) = p(t_0)p(t_1|t_0)p(t_2|t_1)$\n",
    "    <img src=\"Figure_16_2.png\" width=300>\n",
    "* General Case\n",
    "  * $p(\\mathbf{x}) = \\Pi_i p(x_i| Pa_\\mathcal{G}(x_i))$\n",
    "    * $Pa_\\mathcal{G}(x_i)$ means 'Pa'rent nodes of $x_i$ in $\\mathcal{G}$.\n",
    "  \n",
    "* n discrete variables each having k values\n",
    "  * Unstructured Models: $O(k^n)$ parameters\n",
    "  * Structured Models: $O(k^m)$ parameters (m is the maximum number of variables in a single conditional p(...).\n",
    "    * few parents in the graph -> few paramters. -> efficient\n",
    "* Graph encoding\n",
    "  * \"Which variables are Conditionally independent from each other.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.2 Undirected Models\n",
    "\n",
    "* Known as Markov Random Fields(MRFs), Markov Networks\n",
    "* Use when the interactions seem to have no intrinsic direction or to operate in both directions.\n",
    "  * e.g. Health of your roommate, you, your coworker\n",
    "<img src=\"Figure_16_3.png\" width=300>\n",
    "  \n",
    "* General Case\n",
    "  * Unnormalized Probability Distribution for Undirected Models\n",
    "    * $\\tilde{p}(\\mathbf{x})=\\Pi_{C\\in \\mathcal{G}} \\phi(C)$\n",
    "      * C is clique: a subset of nodes connected to each other.\n",
    "      * $\\phi(C)$ is clique potential\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.3 The Partition Function\n",
    "\n",
    "* Normarlized Probability Distribution for Undirected Models\n",
    "  * $p(x) = \\frac{1}{Z}\\tilde{p}(\\mathbf x)$\n",
    "    * $Z = \\int \\tilde{p}(\\mathbf x) dx$\n",
    "* $Z$ is known as the partition function, a term borrowed from statistical physics.\n",
    "  * often intractable to compute.\n",
    "  * $\\phi$ must be conducive to computing $Z$ efficiently.\n",
    "  * We need Approximations (CH 18)\n",
    "  \n",
    "* Difference btw Directed and Undirected Models\n",
    "```\n",
    " Directed models are defined directly in terms of probaility distributions from the start, while undirected models are defined more loosely by $\\phi$ functions that are then converted into probability distributions.\n",
    "```\n",
    "* The domain of each of the variables has dramatic effect on the kind of probability distribution that a given set of $\\phi$ corresponds to.\n",
    "* Often, it is possible to leverage the effect of a carefully chosen domain of a variable in order to obtain complicated behavior from a relatively simple set of $\\phi$.\n",
    "  * e.g. Z diverge or not? $\\mathbf{x} \\in \\mathbb{R}^n or \\{ 0,1 \\}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.4 Energy-Based Models (EBM)\n",
    "\n",
    "* $\\tilde{p}(\\mathbf{x}) = \\exp(-E(\\mathbf{x}))$\n",
    "  * to Make $\\forall{\\mathbf{x}},\\tilde{p}(\\mathbf{x}) > 0$\n",
    "  * $E(\\mathbf{x})$ is Energy Function\n",
    "  * $\\tilde{p}(\\mathbf{x})$ in EBM is an example of a Boltzmann distribution.\n",
    "    * Many energy-based models are called Boltzmann machines.\n",
    "* Different cliques in the undirected graph correspond to the different terms of the energy function.\n",
    "  <img src=\"Figure_16_4.png\" width=600>\n",
    "  <img src=\"Figure_16_5.png\" width=600>\n",
    "  * a EBM is just a special kind of Markov network\n",
    "  * One can view a EBM a Product of \"Experts\"(each term)\n",
    "* The words such as \"Partition function\", \"Energy\" are originally developed by statistical physicists.\n",
    "* Not compute $p_\\text{model}(\\mathbf{x})$ but only $\\log \\tilde{p}_\\text{model}(\\mathbf{x})$\n",
    "  * Free energy with latent variables $\\mathbf{h}$\n",
    "    * $\\mathcal{F} = -\\log \\Sigma_h \\exp(-E(\\mathbf{x}, \\mathbf{h}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.5 Separation and D-Separation\n",
    "* Which variables indirectly interact?\n",
    "* Separation (for Undirected Models)\n",
    "  * Conditionally independence implied by the graph\n",
    "  <img src=\"Figure_16_6.png\" width=600>\n",
    "  * If (1) No path exists between them or (2) all paths contain an observed variable, => Separated\n",
    "  * Active/Inactive\n",
    "    * paths involving only unobserved variables => Active => Not Separated (Dependent)\n",
    "    * all paths including an observed variable => Inactive => Separated (Independent)\n",
    "  <img src=\"Figure_16_7.png\" width=600> \n",
    "* d-Separation (for Directed Models)\n",
    "  * \"d-\" means \"dependence\" \n",
    "  * e.g. Active, Not Separated, Dependent\n",
    "  <img src=\"Figure_16_8.png\" width=600>\n",
    "  * e.g. Active, Not Separated \n",
    "  <img src=\"Figure_16_9.png\" width=600>\n",
    "\n",
    "* Context-specific Independences\n",
    "  * Cannot be represented with Existing Graphical Models\n",
    "  * Independences that are present dependent on the value of some variables in the network.\n",
    "  * e.g.\n",
    "    1. a = 0 => b and c are independent\n",
    "    2. a = 1 => b = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.6 Converting between Undirected and Directed Graphs\n",
    "\n",
    "* We often refer to a specific machine learning model as being undirected or directed.\n",
    "  * RBM: undirected\n",
    "  * Sparse coding: Directed\n",
    "* But Every probability distribution can be represented by either a Directed or an Undirected\n",
    "  * No probabilistic model is inherently directed or undirected.\n",
    "  * Some models are most easily described using a directed (or undirected) graph\n",
    "* We should \"Choose\" which language to use for each task.\n",
    "  * Which approach can capture the most independences.\n",
    "  * Which approach uses the fewest edges\n",
    "* We may sometimes switch between different modeling languages fwith a single probability distribution\n",
    "  * Sampling: Directed (Ch16.3)\n",
    "  * Approximate Inference: Undirect (Ch19)\n",
    "* Conversion between Directed and Undirected\n",
    "  * Converting Directed into Undirected\n",
    "    * Undirected Model cannot represent \"Immorality\" of Directed Model\n",
    "      * Immorality\n",
    "        * Common child of Multiple not-connected parents in Directed Graph\n",
    "      * Moralized Graph\n",
    "        * Undirected Graph for representing independence of Immorality\n",
    "        * Add undirected edge between Immoral parents. \n",
    "  <img src=\"Figure_16_11.png\" width=600>\n",
    "  * Converting Undirected into Directed\n",
    "    * Directed Model Cannot represent Undirected Model with a loop of length greater than 3.\n",
    "    * Chord\n",
    "      * A connection between any two non-consecutive variables in the loop.\n",
    "    * Converting to Chordal or Triangulated Graph\n",
    "      * Remove loops of length greater than 3.\n",
    "      * Make all loops as Triangular loops\n",
    "    * WHY?\n",
    "      * Review \"Separated in Undirected\" and \"d-Separated in Directed\"\n",
    "      * Left: Impossible to convert\n",
    "  <img src=\"Figure_16_12.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.7 Factor Graphs\n",
    "\n",
    "* Another way of drawing Undirected Models\n",
    "  * Resolve an ambiguity in the graphical representation\n",
    "  * Factor graphcs explicitly represent the scope of each $\\phi$.\n",
    "  <img src=\"Figure_16_13.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 Sampling from Graphical Models\n",
    "\n",
    "* Ancestral Sampling\n",
    "  * Use \"ONLY\" Directed Graphical Models\n",
    "  * Sort variables $\\mathbf{x}_i$ in the graph into a topological ordering.\n",
    "  * Sample in this order\n",
    "  * e.g. If $\\mathbf{x}_1$ -> $\\mathbf{x}_2$ -> $\\mathbf{x}_3$ ... -> $\\mathbf{x}_n$\n",
    "    * Sample $\\mathbf{x}_1 \\sim P(\\mathbf{x}_1)$\n",
    "    * Sample $\\mathbf{x}_2 \\sim P(\\mathbf{x}_2 | \\mathbf{x}_1)$\n",
    "    * ...\n",
    "    * Sample $\\mathbf{x}_n \\sim P(\\mathbf{x}_n | \\mathbf{x}_{n-1})$\n",
    "* Ancestral Sampling for Undirected Models\n",
    "  * Preprocessing: Converting Undirected to Directed\n",
    "  * Some undirected Models cannot be converted into Directed\n",
    "  * Some converted Models becomes intractable.\n",
    "* Gibb Sampling for Undirected Models (Ch17)\n",
    "  * Use separation properties\n",
    "  * Iteratively visit each vaiable $\\mathbf{x}_i$\n",
    "  * Not a fair sample $\\mathbf{x} \\sim P(\\mathbf{x})$\n",
    "  * Focus only one variable and \"Local\" condition on only the neighbors sampling\n",
    "  * Repeat!!! -> Converges to sampling from the correct distribution\n",
    "  * But When we stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.4 Advantages of Structured Modeling\n",
    "* Reduce the cost of representing probability distributions as well as learning and inference\n",
    "* Sampling\n",
    "* Easier to develop and debug\n",
    "  * Separate Representation of knowledge from learning of knowledge or inference given existing knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.5 Learning about Dependencies\n",
    "* Use Latent(or Hidden) Variables $\\mathbf{h}$\n",
    "* Visible $\\mathbf{v}$s with only direct connections VS $\\mathbf{v}$s with indirect connections using $\\mathbf{h}$\n",
    "  * Many edges    VS small edges\n",
    "  * Large cliques VS small cliques\n",
    "  * Computation cost High VS Low\n",
    "* \"Structure Learning\" \n",
    "  * Greedy search\n",
    "    * Propose a structure (with a small number of edges added or removed)\n",
    "    * Train it, give a score\n",
    "    * Reward accuracy and penalize model complexity\n",
    "    * Repeat\n",
    "  * Using $\\mathbf{h}$ avoids Descrete searchs and multiple rounds of training. (Effective)\n",
    "    * A fixed structure over $\\mathbf{v}$, $\\mathbf{h}$ enough!!!\n",
    "* $\\mathbf{h}$ also proviede an alternative representation for $\\mathbf{v}$.\n",
    "  * Richer descriptions of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.6 Inference and Approximate Inference\n",
    "* Inference Problem\n",
    "  * Predict the value of some variables given other variables\n",
    "  * Predict the probability distribution over some variables given the value of other variables\n",
    "* Unfortunately inference problems are ...\n",
    "  * Intractable\n",
    "      * a structured graphical model is Not enough to allow efficient inference\n",
    "  * Computing Marginal probability of a general graphical model is NP Hard. ( Very Hard to solve )\n",
    "* Use approximate distribution $q(\\mathbf{h}|v) \\approx p(\\mathbf{h}|v)$ (Ch19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.7 The Deep Learning Approach to Structured Probabilistic Models\n",
    "\n",
    "* Deep Graphical Models VS Traditional Graphical Models\n",
    "\n",
    "\n",
    "* Different design decisions for Deep Learning\n",
    "  * Different algorithms\n",
    "  * Different models\n",
    "* The depth of a model in Deep Graphical Models\n",
    "  * Depth of latent varaible $\\mathbf{h}$ is...\n",
    "    * The shortest path from $\\mathbf{h}$ to an observed variable.\n",
    "* Deep Learning models...\n",
    "  * Use Distributed Representations\n",
    "  * Typically have more latent variables than observed variables.\n",
    "  * Focus Indirect effect\n",
    "    * Capture Nonlinear interactions via Indirect connections between multiple latent variables.\n",
    "* Traditional Graphical Models\n",
    "  * Focus Direct effect\n",
    "    * Capture Nonlinear interactions using High order term and Structure Learning between variables.\n",
    "    * Use only few latent variables\n",
    "    \n",
    "* Design of Latent variables in Deep Graphical Models\n",
    "  * The latent variables do not have any specific semantics\n",
    "  * Usually not very easy for a numan to inteprete\n",
    "  * c.f. In the traditional models...\n",
    "    * Latent variables are designed with some specific semantics in \"human\" mind.\n",
    "    * Less able to scale to complex problems\n",
    "    * Not reusable\n",
    "\n",
    "* Connectivity typically used in Deep Graphical Models\n",
    "  * Have large groups of units that are all connected to other groups of units.\n",
    "    * Interactions between two groups may be described by a single matrix.\n",
    "    * c.f. In the traditional models...\n",
    "       * few connections and the choice of connections for each variable.\n",
    "\n",
    "* Training Algorithms is \"free\" to model a particular dataset.\n",
    "  * c.f. In the traditional models...\n",
    "    * The choice of inference algorithm is \"tightly linked\" with the design of the model structure.\n",
    "    \n",
    "* Traditional approaches typically aim to \"Exact inference\".\n",
    "  * Or use \"loopy belief propagation\" for approximate inference algorithm. (Murphy Ch20)\n",
    "  * c.f. In Deep Graphical Model..\n",
    "    * Gibbs sampling or variational inference algorithms.\n",
    "    \n",
    "* Both approaches work well with very Sparsely connected graphs (using exact inference or loopy belief propatation).\n",
    "  * In case that the graphs are not spase enough\n",
    "    * exact inference or loopy belief propagation are not relevant.\n",
    "    \n",
    "* Deep Graphical Models In the view of Computation\n",
    "  * A very large latent variables makes efficient numerical code essential.\n",
    "    * => implemented with efficient matrix product operations\n",
    "    * => sparsely connected generalizations \n",
    "      * block diagonal matrix products or convolutions\n",
    "    * c.f. Traditional Model use one big matrix.(?)\n",
    "    \n",
    "* Trend: The deep learning approach is often...\n",
    "  * Figure out what the minimum amount of information we absolutely need\n",
    "  * Figure out how to get a reasonable approximation of that information as quickly as possible.\n",
    "  * c.f. Traditional approach ...\n",
    "    * Simplifying the model until computing exactly.\n",
    "  * Increase power of the model until it is barely possible to train or use.\n",
    "    * We train Model!!!\n",
    "      * Marginal distributions cannot be computed\n",
    "        * However Satisfied to draw approximate samples\n",
    "      * Objective function is intractable. \n",
    "        * However have an estimate of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.7.1 Example: The Restricted Boltzmann Machine\n",
    "\n",
    "<img src=\"Figure_16_14.png\" width=400>\n",
    "* Divied into 2 groups $\\mathbf{v}$, $\\mathbf{h}$\n",
    "  * No direct interactions between any $\\mathbf{v}$ or any $\\mathbf{h}$ (Restricted) \n",
    "* Used for Deep learning\n",
    "* Energy function (Induced in Ch20.2)\n",
    "$$\\tilde{p}(\\mathbf{v,h}) = \\exp(-E(\\mathbf{v,h}))$$\n",
    "$$p(\\mathbf{v,h}) = \\frac{1}{\\mathbf{Z}} \\exp(-E(\\mathbf{v,h}))$$\n",
    "$$E(\\mathbf{v},\\mathbf{h})= -\\mathbf{b}^{T} \\mathbf{v}-c^{T} \\mathbf{h} - \\mathbf{v}^{T}\\mathbf{W}\\mathbf{h}$$\n",
    "  * Properties\n",
    "$$p(\\mathbf{h}|\\mathbf{v})=\\Pi_i p(\\mathrm{h}_i| \\mathbf{v})$$\n",
    "$$p(\\mathbf{v}|\\mathbf{h})=\\Pi_i p(\\mathrm{v}_i| \\mathbf{h})$$\n",
    "$$P(\\mathbf{h}_i = 1| \\mathbf{v}) = \\sigma(\\mathbf{v}^{T} \\mathbf{W}_{:,i} + \\mathrm{c}_{i})$$\n",
    "$$P(\\mathbf{h}_i = 0| \\mathbf{v})= 1 - \\sigma(\\mathbf{v}^{T} \\mathbf{W}_{:,i} + \\mathrm{c}_{i})$$\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{W}_{i,j} }E(\\mathbf{v}, \\mathbf{h}) = -\\mathrm{v}_i \\mathrm{h}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
