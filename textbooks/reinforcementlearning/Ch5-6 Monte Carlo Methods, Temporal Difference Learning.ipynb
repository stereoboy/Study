{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch5 Monte Carlo Methods\n",
    "\n",
    "* Previous: Evaluation by Dynamic Programming\n",
    "  * Counting all possible situation\n",
    "* Computing Evaluation by Sampling(Monte Carlo)\n",
    "\n",
    "* Behavior Policy for visiting all situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ch6 Temporal Difference Learning\n",
    "\n",
    "> As usual, we start by focusing on the policy evaluation or prediction problem, that\n",
    "of estimating the value function vπ for a given policy π. For the control problem\n",
    "(finding an optimal policy), DP, TD, and Monte Carlo methods all use some variation\n",
    "of generalized policy iteration (GPI). The differences in the methods are primarily\n",
    "differences in their approaches to the prediction problem.\n",
    "\n",
    "* Both TD and Monte Carlo methods use experience to solve the prediction problem.\n",
    "\n",
    "* $V(S_t)$ Update in Monte Carlo\n",
    "  * $V(S_t) \\leftarrow V(S_t) + \\alpha [ G_t - V(S_t)]$\n",
    "* $V(S_t)$ Update in TD\n",
    "  * $V(S_t) \\leftarrow V(S_t) + \\alpha [ R_t + \\gamma V(S_{t+1}) -  V(S_t)]$\n",
    "  * Use $R_t + \\gamma V(S_{t+1})$ as estimation of $G_t$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
