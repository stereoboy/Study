{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model and Planning\n",
    "\n",
    "* We use \"PLANNING\" to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment\n",
    "\n",
    "\n",
    "* state-space planning\n",
    "  * a search through the state space for an optimal policy or path to a goal.\n",
    "* Plan-space planning\n",
    "  * Operators transform one plan into another, and value functions, if any, are defined over the space of plans.\n",
    "  * Plan-space methods are difficult to apply efficiently to the stochastic optimal control problems that are the focus in reinforcement learning, and we do not consider them further (see, e.g., Russell and Norvig, 2010).\n",
    "* In this chapter we argue that various other state-space planning methods also fit this structure, with individual methods differing only in the kinds of backups they do, the order in which they do them, and in how long the backed-upinformation is retained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"V7B9ITL6XHLKN8UJ77J17QY11H552RRK.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"A9J0FCP1WU1XH3STLLGRHWST858S5F9X.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna: Integrating Planning, Acting, and Learning\n",
    "\n",
    "* trial-and-error learning\n",
    "  * importance of cognition\n",
    "* reactive decision-making\n",
    "  * deliberative planning\n",
    "  \n",
    "* random-sample one-step tabular Q-planning\n",
    "\n",
    "* one-step tabular Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"E6GMQ2WSBQ94V9UPD8HQLW6BIYL6BCEA.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If n == 0\n",
    "* Just direct learning one-step Q-learning\n",
    "* except for the last step of the first episode Q table entries are not updated, remained random.\n",
    "\n",
    "### If n == 50\n",
    "* After last step (updating Q as \"Up\", (f) update other <50 Q by simulating Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The general problem here is another version of the conflict between exploration and\n",
    "exploitation. In a planning context, exploration means trying actions that improve\n",
    "the model, whereas exploitation means behaving in the optimal way given the current\n",
    "model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
