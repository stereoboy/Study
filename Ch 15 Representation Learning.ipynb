{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "* Tools\n",
    " * Ch13. Linear Factor Models\n",
    " * Ch14. Autoencoders\n",
    "* Overviews(?)\n",
    " * Ch15. Representation Learning\n",
    "* Specific Issues\n",
    " * Ch16. Structured Probabilistic Models for Deep Learning\n",
    " * Ch17. Monte Carlo Methods\n",
    " * Ch18. Confronting the Partition Function\n",
    " * Ch19. Approximation Inference\n",
    " * Ch20. Deep Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Introduction\n",
    " * Definition of Representation\n",
    "* Greedy Layer-Wise Unsupervised Pretraining\n",
    " * When and Why Does Unsupervised Pretraining Work?\n",
    "* Transfer Learning and Domain Adaptation\n",
    " * Use shared representation\n",
    "* Semi-Supervised Disentangling of Casual Factors\n",
    " * Use information from unsupervised tasks to perform supervised task\n",
    "* Distributed Representation\n",
    "* Exponential Gains from Depth\n",
    " * Deep representation\n",
    "* Providing Clues to Discover Underlying Causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Representation\n",
    "* Arabic numeral representation VS Roman numeral representation\n",
    " * 210 / 6  VS  CCX / VI\n",
    "* Better representation in Machine Learning\n",
    " * Good one makes a subsequent task easier\n",
    "* Almost learning algorithms learns \"Representations\" in the Deep Architecture\n",
    " * Supervised/Unsupervised Learning learns \"implicitly\" as side effects\n",
    " * Some algorithms designed explicitly for Representation Learning\n",
    "   * e.g. Distribution Learing (Density Estimation)\n",
    "* Tradeoff Issue\n",
    " * Preserving much information VS Nice properties (e.g. Independence)\n",
    "\n",
    "## Use Unlabeled Data for a good representation\n",
    "* Unsupervised Learning\n",
    "* Semi-suprevised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Layer-Wise Unsupervised Pretraining\n",
    "* Greedy Layer-Wise?\n",
    " * optimizes each layer at a time rather than jointly optimizing all pices\n",
    " \n",
    "* Use single-layer representation learning algorithm\n",
    " * RBM, single-layer autoencoder, sparse coding model (Ch13/14)\n",
    " * Take the output of the previous layer\n",
    " * Produce a new simpler representation\n",
    "\n",
    "<img src='Algorithm_15_1.png' width=800>\n",
    "\n",
    "* Good Initialization for a joint learning procedure over all the layers of a deep neural net for supervised task\n",
    "* Used to successfully train \"even\" fully connected architectures\n",
    "\n",
    "\n",
    "* Fine tuning after pretraining\n",
    " * Optimizes all layers together\n",
    " * Can be done in the pretraining phase (pretraining & fine-tuning simultaneously)\n",
    "* Can be viewed as a regularizer in supervised learning task\n",
    "* Overall training scheme is nearly the same\n",
    " * learning algorithms, model types can differ\n",
    "\n",
    "\n",
    "* Initialization for unsupervised learning algorithms for...\n",
    " * Deep autoencoders\n",
    " * Probailistic models with many layers of latent variables\n",
    " * Deep Generative Models (Ch20)\n",
    "   * Deep belief networks\n",
    "   * Deep Boltzmann machines\n",
    "   \n",
    "## When and Why Does Unsupervised Pretraining Work?\n",
    "\n",
    "* History\n",
    " * Substantial improvements in test error for \"Classification Tasks\"\n",
    "   * Revival of deep neural networks (2006, Hinton)\n",
    " * Harmful on many other tasks\n",
    " * Ma,J. (2015, Deep neural nets as a method for quantitative sturucture) found...\n",
    "   * Significantly helpful for many tasks\n",
    "   * Slightly harmful on average\n",
    " * So we should know \"When and Why pretraining works\" for a particular task\n",
    "\n",
    "\n",
    "* 2 Intuitions\n",
    " * Act as regularizer\n",
    "  * e.g. Optimize only higher layers(classifier) freezing lower layers (feature extractor) \n",
    "  * Prevent overfitting\n",
    "  * Improve test set error\n",
    "  * Speed up optimization\n",
    " * Some features that are useful for the unsupervised task may also be useful for the supervised learning task\n",
    "  * After extracting wheels, we can classify cars and motorcycles by counting wheels\n",
    "\n",
    "* Expected Values\n",
    " * More effective when the initial input is poor\n",
    "   * dimension reduction + manifold learning (Ch14)\n",
    "   * e.g. good similarity metrics between two words for word embeddings\n",
    " * User unlabeled data when labeled data is very small (Semi-supervised learning)\n",
    " * Regularization for complicated functions\n",
    " \n",
    " \n",
    "* Why it works\n",
    " * reduce the viraince of the estimation process\n",
    "   * Figure 15.1 explanation\n",
    "     * Input-output projection for visualization \n",
    "     * variaous starting points (initialization)\n",
    "     * blue -> red: time line, from origin to outside\n",
    "     * points based on pretraing move to small region\n",
    "\n",
    "<img src='Figure_15_1.png' width=800>\n",
    "\n",
    "* Comparison to other ways\n",
    " * Two \"separate\" phases\n",
    " * Increase hyperparameters => time consuming\n",
    " * => one phase pretraining\n",
    "   * Unsupervised learning and supervised learning simultaneously\n",
    "   * Attach unsupervised learning term to objective function\n",
    "* Two phase VS one phase\n",
    " * many hyperparams vs single hyperparam\n",
    " * several trial-error iteration vs one-shot\n",
    " * no way to control regularization term vs control it by the coefficient of unsupervised cost term \n",
    "\n",
    "* The popularity of unsupervised pretraining has declined\n",
    " * Still popular in NLP(natural language processing)\n",
    " * Regualized with dropout or Batch normalization for classification\n",
    "   * outperform pretraing versions on even medium-size datasets\n",
    " * Bayesian methods outperform on small datasets\n",
    " \n",
    " \n",
    " \n",
    "* Nevertheless unsupervised pretraining...\n",
    " * an important milestone in the history of deep learing research\n",
    " * continues to influence contemporary approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Domain Adaptation\n",
    "\n",
    "* One example problelm of Transfer learning\n",
    " * How to use feature extractor from Zebra vs Horse for classification of Dalmatian vs Dog\n",
    " \n",
    "* In transfer learning, the learner must perform two or more different tasks\n",
    " * e.g. Learn on significantly more data (P1), apply the learned transformation on P2(Small data)\n",
    " \n",
    "\n",
    "* Sharing layers\n",
    " * Share lower layers (Underlying factor in low level feature) => Multi-task learning\n",
    "   * e.g. Visual categorizing\n",
    "     * low-level notions of \"Edges\" and \"Visual shapes\" (corner? circle?)\n",
    " <img src='Figure_7_2.png' width=800>\n",
    " * Share higher layers (Speech recognitoin) => Domain Adaptation\n",
    " <img src='Figure_15_2.png' width=800>\n",
    " \n",
    "* Domain Adaptation (Sharing Higher Layer)\n",
    " * Same task -> Different distrtibution P\n",
    "  * e.g. Learning positive/Negative sentiment\n",
    "    * Task1: about Music, Task2: about Movies\n",
    "    * Why?: vocabulary and style vary from one domain to another\n",
    "\n",
    "\n",
    "* Concept Drift\n",
    " * Gradual changes in the data distribution over time\n",
    " \n",
    "---\n",
    "```\n",
    "While the phrase \"multi-task learning\" typically refers to supervised learning tasks, the more general notion of transfer learning is applicable to unsupervised learning and reinforcement learning as well.\n",
    "```\n",
    "----\n",
    "\n",
    "* Same representation may be useful in both settings\n",
    " * e.g. Transfer learning competition\n",
    "   * Mesnil, G. 2011, Unsupervised and tranfer learning challenge: a deep learing approach\n",
    "   * 1st: Learn on $P_1$\n",
    "   * 2st: Apply the learned transformation to $P_2$\n",
    "   * Result\n",
    "     * deeper representations => faster learning $P_2$\n",
    "\n",
    "* Two examples: One-shot learning and zero-shot(zero-data) learning\n",
    " * Extreme forms of transfer learning\n",
    " * One-shot: One example in the 2nd stage\n",
    "   * e.g. \n",
    "     * learn \"wheels\" from images of bikes n cars\n",
    "     * learn the one image of a 3-wheel bike\n",
    "     * test on images of 3-wheel bikes\n",
    " * Zero-shot\n",
    "   * Testing without data in the 2nd stage???\n",
    "   * Learn 2 representations and their relation\n",
    "   * e.g. Text-Image learning\n",
    "     * Link text space(\"4 Legs\") - Image space(visual shape of legs and their count)\n",
    "     * Learn Birds(\"2 Legs\", \"No Ear\"), Dogs(\"2 Legs\", \"Round Ears\")\n",
    "     * Input: Text about Cats (4 Legs, Pointy ears)\n",
    "     * Apply to the images of Cats\n",
    "   * e.g. Machine translation\n",
    "     * We can translate sentences even though some word has no label\n",
    "     * X in language A - Y in language B have similar behavior => Same meaning    \n",
    " \n",
    " \n",
    " <img src='Figure_15_3.png' width=600>\n",
    "\n",
    "\n",
    "* Zero-shot Model\n",
    " * $P(y| x, T)$\n",
    "   * Traditional input $x$\n",
    "   * Traditional output $y$\n",
    "   * Additional random variables, Task $T$\n",
    "   * e.g. $x$ is descriptions about cats, $y$ is \"yes\" or \"no\", $T$ is \"Is there a cat in this image?\"\n",
    " \n",
    "    ---\n",
    "    ```\n",
    "If we have a training set containing unsupervised examples of objects that live in the same space as T , we may be able to infer the meaning of unseen instances of T.\n",
    "    ```\n",
    "    ---\n",
    "\n",
    "    * $T$ should be represented in a way that allows some of generalization.\n",
    "      * \"Is there a sort of \"animals\" in this image?\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Disentangling of Causal Factors\n",
    "* Large amount of unlabeled data and relatively little labeled data\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/d/d0/Example_of_unlabeled_data_in_semisupervised_learning.png width=300>\n",
    "\n",
    "* $P(x)$ is helpful for $P(y|x)$\n",
    "* Causal Factor -(Representation)-> Feature\n",
    "\n",
    "<img src=\"Figure_15_4.png\" width=600>\n",
    "\n",
    "\n",
    "* Better Representations?\n",
    "  1. Representation disentangles the causes from one another\n",
    "  2. Easy to model\n",
    "    * e.g. Simple model: sparsity, independence\n",
    "  \n",
    "* Hypothesis motivation of Semi-supervised learning\n",
    "  * If (1), (2) conside =>\n",
    "  * If a representation $h$ represents many of the underlying causes of the observed $x$\n",
    "    * the outputs $y$ are among the most \"salient\" causes, then it is easy to predict $y$ from $h$.\n",
    "    * $P(y|x)$, $P(x|h)$, $P(h)$\n",
    "  * c.f. If $P(x)$ is uniformly distributed => Semi-supervised learning fails\n",
    "  * Simple example\n",
    "    * \n",
    "  \n",
    "\n",
    "* Issus: Hard to capture salient factors\n",
    "  * Two Strategy\n",
    "    1. Use a supervised learning signal (labeld data)\n",
    "    2. Use much larger representation\n",
    "    \n",
    "    \n",
    "* Adversarial Framework (CH 20)\n",
    "  * Modify the definition of which underlying causes are most salient.\n",
    "  \n",
    "  \n",
    "<img src=\"Figure_15_6.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Distributed Representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Gains from Depth\n",
    "\n",
    "## Deep representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providing Clues to Discover Underlying Causes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
